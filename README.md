# English-to-Occitan-NMT-using-Distillation
A first attempt at exploring viability of using smaller LMs with distillation from larger LMs for low-resource languages. For the current project, I have focussed on the English to Occitan translation direction using a distillation setup with Mistral 7B v0.3 as my teacher model and TinyLlama 1.1B Chat v1.0 as my student model. The performance of both the finetuned teacher and student after distillation was compared on a holdout set with the MarianNMT 200B baseline model.
As this repo is under maintenance and overhaul, please use the .ipynb notebook for an initial proof of reference. Subsequent versions would feature a modular structure for ease of use and reproducibility, as well as detailed metrics and plots for evaluation. I am also working on proper attribution for open source models and repos used, so I apologise for the lack of acknowledgements in the current readme.
