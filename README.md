# English-to-Occitan-NMT-using-Distillation

A first attempt at exploring viability of using smaller LMs with distillation from larger LMs for low-resource languages. For the current project, I have focussed on the English to Occitan translation direction using a distillation setup with Mistral 7B v0.3 as my teacher model and TinyLlama 1.1B Chat v1.0 as my student model. The performance of both the finetuned teacher and student after distillation was compared on a held-out test set extracted from FLORES-200 with a Huggingface variant of the MarianNMT 200B baseline model finetuned for Romance languages.

As this repo is under maintenance and overhaul, please use the .ipynb notebook for an initial proof of concept. The notebook can be run from start to end with cells after the evaluation part not to be considered, as those are for a separate experiment. Subsequent versions would feature a modular structure for ease of use and reproducibility, as well as detailed metrics and plots for evaluation. I am also working on proper attribution for open source models and repos used, so I apologise for the lack of acknowledgements in the current README file.
